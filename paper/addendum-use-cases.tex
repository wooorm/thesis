%*******************************************************
% Addendum Use Cases
%*******************************************************

\begingroup
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

% work-around to have small caps also here in the headline
\manualmark
\markboth{\spacedlowsmallcaps{Use Cases}}{\spacedlowsmallcaps{Use Cases}}

\noindent Due to strict requirements for the format of this document,
  such as page count, numerous topics of the process were not put on paper.
This includes the omission of detailed information on the process behind
  drafting the target audience's use cases, how I developed a proposal that
  enables the target audience to solve their use cases, the validation of the
  target audience's reception, and in-depth coverage of the written code.
These addenda delve deeper into the first three: use cases, from use cases
  to a proposal, and validation.

\chapter*{Use Cases}\label{addendum-use-cases}
\addtocontents{toc}{\protect\vspace{\beforebibskip}}
\addcontentsline{toc}{chapter}{\tocEntry{Use Cases}}

In essence, the target audience's use cases were defined by looking at what
  challenges current \gls{ecmascript} implementations cover.
What the target audience would \emph{not} employ an implementation for, was
  defined by looking at the unique challenges covered by non-\gls{ecmascript}
  implementations.

In the pre-thesis process, the implementation was design-focussed.
Initially, as described in the introduction (p.\,\pageref{introduction}), I
  was intrinsically motivated to create a ``better'' solution for typography
  on the web.
Current solutions did not provide sufficient functionality and neither did my
  initial solution.
The parser and data representation of the initial solution were incomplete.
Thus, during the development of the thesis, my focus shifted from design to
  natural language in general.

I searched for solutions covering general \gls{nlp}.
For open source projects, I searched GitHub.
In any language, I searched for popular ``natural language''
  projects\footnote{
    \href{https://github.com/search?o=desc&q=natural+language&s=stars}{\nolinkurl{github.com/search?o=desc&q=natural+language&s=stars}}
  }.
For web projects, I added a programming language filter for
  \gls{ecmascript}\footnote{
    \href{https://github.com/search?l=JavaScript&o=desc&q=natural+language&s=stars}{\nolinkurl{github.com/search?l=JavaScript&o=desc&q=natural+language&s=stars}}
  }.

To find out how non-\gls{ecmascript} implementations solved \gls{nlp}
  challenges I read \emph{Natural Language Processing
  Python}\footnote{
    \href{http://www.nltk.org/book/}{\nolinkurl{www.nltk.org/book/}}
  }, a book which describes how to approach numerous natural language
  challenges with \textsc{nltk}, a Python project for \gls{nlp}.
Additionally, WikiPedia's list of \gls{nlp} tool-kits\footnote{
    \href{http://en.wikipedia.org/wiki/List_of_natural_language_processing_toolkits}{\nolinkurl{en.wikipedia.org/wiki/List_of_natural_language_processing_toolkits}}
  } was of great help in discovering other projects, such as Open\textsc{nlp}
  by Apache, Core\textsc{nlp} by Stanford, and \textsc{gate}.

Additionally, I read numerous papers on the subject (of which some were used in
  the thesis, see \bibname, p.\,\pageref{works-cited}), and talked with
  Vasilis van Gemert, an expert on web design and development.
Both of which, I argue, are expert-based design, and in turn user-centred
  design.

The discovered implementations each provide functionality to accomplish
  certain \gls{nlp} challenges.
Whether parsing dates, creating summaries, detecting sentiment, or everything
  together, they solve the target audience's use cases.

The following table shows \gls{nlp} tasks according to
  WikiPedia\footnote{
    \href{http://en.wikipedia.org/wiki/Natural_language_processing}{\nolinkurl{en.wikipedia.org/wiki/Natural_language_processing}}
  }, and indicates if a client side implementation exists.
\medskip

{\footnotesize
\begin{tabular}{ l p{5.25cm} }
  Challenge &
  Implementation
  \\ \hline
  Automatic summarisation &{\raggedright
  \href{https://github.com/jbrooksuk/node-summary}{\nolinkurl{github.com/jbrooksuk/node-summary}}
  }\\
  Coreference resolution &
  \\
  Discourse analysis &
  \\
  Machine translation &
  \\
  Morphological segmentation &
  \\
  Named entity recognition &
  \\
  Natural language generation &{\raggedright
  numerous
  }\\
  Natural language understanding &{\raggedright
  \href{https://github.com/wanasit/chrono}{\nolinkurl{github.com/wanasit/chrono}}
  }\\
  Optical character recognition &{\raggedright
  \href{https://github.com/antimatter15/ocrad.js}{\nolinkurl{github.com/antimatter15/ocrad.js}}
  }\\
  \gls{pos} tagging &{\raggedright
  \href{https://github.com/dariusk/pos-js}{\nolinkurl{github.com/dariusk/pos-js}}
  }\\
  Parsing &{\raggedright
  \href{https://github.com/srmor/text-parse}{\nolinkurl{github.com/srmor/text-parse}}
  }\\
  Question answering &
  \\
  Relationship extraction &
  \\
  Sentence breaking &{\raggedright
  numerous
  }\\
  Sentiment analysis &{\raggedright
  numerous
  }\\
  Speech recognition &
  \\
  Speech segmentation &
  \\
  Topic segmentation &
  \\
  Topic recognition &
  \\
  Word segmentation &{\raggedright
  numerous
  }\\
  Word sense disambiguation &
  \\
  Stemming &{\raggedright
  numerous
  }\\
  Text simplification &
  \\
  Text-to-speech &
  \\
  Text-proofing &{\raggedright
  \href{https://github.com/btford/write-good}{\nolinkurl{github.com/btford/write-good}}
  }\\
  Natural language search &{\raggedright
  \href{https://github.com/visionmedia/reds}{\nolinkurl{github.com/visionmedia/reds}}
  }\\
  Query expansion &{\raggedright
  \href{https://github.com/visionmedia/reds}{\nolinkurl{github.com/visionmedia/reds}}
  }\\
  Automated essay scoring &{\raggedright
  \href{https://github.com/NaturalNode/natural}{\nolinkurl{github.com/NaturalNode/natural}}
  }\\
  Truecasing &{\raggedright
  \href{https://github.com/nbubna/Case}{\nolinkurl{github.com/nbubna/Case}}
  }\\
\end{tabular}
}

\medskip\noindent This list includes use cases \ref{list:use-case:1},
  \ref{list:use-case:2}, \ref{list:use-case:3}, \ref{list:use-case:7}
  (resp. automatic summarisation, generation, sentiment analysis, and
  search).
Additionally, In my opinion, use case \ref{list:use-case:6}, language
  recognition, implemented by
  \href{https://github.com/richtr/guessLanguage.js}{\nolinkurl{github.com/richtr/guessLanguage.js}},
  should be listed.
These challenges all feature implementations for the client side.

The other use cases, \ref{list:use-case:4} and \ref{list:use-case:5} (resp.
  typographic enhancements and counting units), are not on WikiPedia's list as
  they serve little academic use, but are implemented numerous times on the
  web.

The \gls{nlp} challenges from WikiPedia not listed in Use Cases
  (ยง\,\ref{use-cases}, p.\,\pageref{use-cases}), are truecasing,
  stemming, understanding, text-proofing, query expansion, essay scoring,
  optical character recognition, \gls{pos} tagging, parsing,
  sentence breaking, and word segmentation.
The last three were not listed in the use cases as they are solved by the
  proposal by default.
The first two were deemed overly simple as numerous good implementations exist
  for the target audience.
All other tasks were deemed not practical enough for the target audience.

\endgroup
