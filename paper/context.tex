\chapter{Context}\label{context}

% Reset glossary expansions.
\glsresetall

\section{Natural Language Processing}\label{natural-language-processing}

\begin{quote}
  \textit{Natural Language Processing is a theoretically motivated range
    of computational techniques for analyzing and representing
    naturally occurring texts at one or more levels of linguistic
    analysis for the purpose of achieving human-like language processing
    for a range of tasks or applications.
  }

  \medskip
  --- Elizabeth D. Liddy \autocite*{natural-language-processing-liddy-2001}
\end{quote}

\noindent The focus of this paper is \gls{nlp}.
\gls{nlp} is a field related to human--computer interaction, as it
  concerns itself with enabling machines to understand human language.
Human language, a medium which is easy for humans to understand, poses
  problems for machines.
The Georgetown--\gls{ibm} experiment in 1945, one of the first application
  of \gls{nlp}, illustrates this difficulty.
During this study in New York, scientists demonstrated a
  Russian--English translation system
  \autocite{hutchins-john-georgetown-ibm-system}.
The machine translated more than sixty sentences from Russian to English.
The experiment was well publicised and resulted in optimism.
The public believed machine translation would be a ``solved problem'' within
  three to five years.
Despite promising first results, the following ten years were disappointing
  and led to reduced funding.

Machine translation is just one of many major tasks involved with \gls{nlp}.
Other tasks include generating summaries, detecting references to people
  and places, or extracting opinion.
Many programs exist to carry out these and many other \gls{nlp} tasks.
The approach taken to perform these tasks are often similar between
  implementations.
For example, Entity linking is often implemented as follows
  \autocite[according to][]{stanbol-enhancer-nlp}:

\begin{enumerate}
\item\emph{Language Detection (optional)} --- Based on the language of the
    given text, the algorithms behind the following steps will change.
  Omitted if the implementation supports a single language;
\item\emph{Sentence Tokenisation (optional)} --- Sentence breaking elevates
    performance and heightens accuracy of the following stages, in
    particular \acrshort{pos} tagging;
\item\emph{Word Tokenisation} --- The entities (words) must be free from
  their surroundings;
\item\emph{\acrfull{pos} Tagging (optional)} --- It is often desired to link
    several nouns or proper nouns.
  Detecting word categories makes this achievable;
\item\emph{Noun Phrase Detection (optional)} --- Although \emph{apple} and
    \emph{juice} could be two entities, it is more appropriate to link to
    one entity: \emph{apple juice}.
  Detecting noun phrases makes this possible;
\item\emph{Lemmatisation or Stemming (optional)} --- Be it \emph{walk},
    \emph{walked}, or \emph{walking}, all forms of \emph{walk} could link
    to the same entity.
  Detecting either makes this possible;
\item\emph{Entity Linking} --- Linking detected entities to references, such
    as an encyclopaedia.
\end{enumerate}

\noindent \gls{nlp} covers many tasks, but the process of accomplishing these
  goals touches, as seen above, on well-defined stages.

\section{Scope}\label{scope}

Although many \gls{nlp} tasks exist, the standard and the implementation
  this paper proposes will only cover one: \emph{tokenisation}.
Tokenisation, as defined here, includes breaking sentences, words, and
other grammatical units.

Another limitation of the scope of the proposal, is that it focusses on
  syntactic grammatical units. Thus, semantic units (i.e., phrases and
  clauses) are ignored.

In addition, the paper focusses on written language (text), thus ignoring
  spoken language.

Last, this paper focusses on Latin script languages: written languages
  using an alphabet based on the classical Latin alphabet.

\section{Implementations}\label{implementations}

While researching algorithms to tokenise natural language few viable
  implementations were found.
Most algorithms look at either sentence- or word tokenisation (rarely both).
This section describes the current implementations, where they excel, and
  what they lack.

\subsection{Stages}\label{stages}

This section delves into how current implementations accomplish
  tokenisation tasks.

\subsubsection{Sentence tokenisation}\label{sentence-tokenisation}

Often referred to as sentence boundary disambiguation\footnote{Both
    sentence tokenisation and sentence boundary disambiguation detect
      sentences.
    Sentence boundary disambiguation focusses on the position where
      sentences break (as in, ``One sentence?\textbar{} Two
      sentences.\textbar{}'', where the pipe symbols refer to the end of
      one sentence and the beginning of another). Sentence
      tokenisation targets both the start and end positions (as in,
      ``\{One sentence?\} \{Two sentences.\}'', where everything between
      braces is classified as a sentence).}, sentence
  tokenisation is an elementary but important part of \gls{nlp}.
It is almost always a stage in \gls{nlp} applications and not an end goal.
Sentence tokenisation makes other stages (e.g., detecting plagiarism or
  \gls{pos} tagging) perform better.

Often, sentences end in one of three symbols: either a full stop (.),
  an interrogative point (?), or an exclamation point (!)\footnote{One
    could argue the in 1962 introduced
    obscure interrobang (â€½), used to punctuate rhetorical statements where
    neither the question nor exclamation alone exactly serve the writer
    well, should be in this list \autocite{interrobang-mks.com}.}.
But detecting the boundary of a sentence is not as simple as breaking it at
  these markers: they might serve other purposes.
Full stops often occur in numbers, suffixed to abbreviations or titles,
  in initialisms\footnote{Although
    the definition of initialism is ambiguous, this paper defines its use
    as an acronym (an abbreviation formed from initial components, such as
    ``sonar'' or ``\textsc{fbi}'') with full stops depicting elision (such as
    ``e.g.'', or ``K.G.B.'').},
  or in embedded content\footnote{Embedded
    content in this paper refers to an external (ungrammatical) value
    embedded into a grammatical unit, such as a \textsc{url} or an emoticon.
    Note that these embedded values often consist of valid words and
    punctuation marks, but almost always should not be classified as such.}.
The interrogative- and exclamation points too can occur ambiguously, such as
  in a quote (e.g., `\,``Of course!'', she screamed').

Disambiguation gets even harder when these exceptions \emph{are} in fact a
  sentence boundary (double negative), such as in
  ``\ldots{}use the feminine form of idem, ead.'' or in
  `\,``Of course!'', she screamed, ``I'll do it!''\,', where in both
  cases the last terminal marker ends the sentence.

\subsubsection{Word tokenisation}\label{word-tokenisation}

Like sentence tokenisation, word tokenisation is another elementary but
important stage in \gls{nlp} applications. Whether stemming, finding
phonetics, or \gls{pos} tagging, tokenising words is an important
precursory step.

Often implementations see words as everything that is \emph{not} white
  space (i.e., spaces, tabs, feeds) and their boundaries as everything that
  is \autocite{loadfive/knwl-source-code}.

Some implementations take punctuation marks into account as boundaries.
This practice has flaws, as it results in the faulty classification of
  inner-word punctuation\footnote{Many such inner-word symbols exist, such
    as hyphenation points, colons (``12:00''), or elision (whether denoted
    by full stops, ``e.g.''; apostrophes, the Dutch ``'s''; or slashes,
    ``N/A'').}
  as part of the surrounding word \autocite{NaturalNode/natural-source-code}.

\subsection{Tasks}\label{tasks}

The previous section covered implementations that solve tokenisation stages
  in \gls{nlp} applications, such as Natural's word
  tokenisers \autocite{NaturalNode/natural-source-code}.
Concluded was that these implementations are lacking.
This section covers several implementations that solve these stages
  as part of a larger task.

\subsubsection{Sentiment Analysis}\label{sentiment-analysis}

Sentiment analysis is an \gls{nlp} task concerned with the polarity
  (positive, negative) and subjectivity (objective, subjective) of text.
It could be part of an implementation to detect messages with a certain
  polarity.
Twitter allows its users to search on polarity.
For example, when a user searches for ``movie :)'', Twitter searches for
  positive tweets.

Sentiment analysis could be implemented as follows:

\begin{enumerate}
\item\emph{Detect Language (optional)};
\item\emph{Sentence Tokenisation (optional)} --- Different sentences have
    different sentiments, tokenising them helps provide better results;
\item\emph{Word Tokenisation} --- Needed to compare with the database;
\item\emph{Lemmatisation or Stemming (optional)} --- Helps classification;
\item\emph{Sentiment Analysis}.
\end{enumerate}

\noindent Sentiment analysers typically include a
  database mapping either words, stems, or lemmas to their respective
  polarity and\slash or subjectivity\footnote{For example, the \textsc{afinn}
    database mapping words to polarity \autocite{nielsen-finn-arup-afinn}.}
  and return the average sentiment per sentence, or for the document.

In the case of the previously mentioned Twitter example, the service filters
  out all neutral and negative results, and return the remaining (positive
  attitude) results.

Many implementations exist for this task
  \autocites{thinkroth/sentimental-source-code}{mileszim/sediment-source-code}
  {thisandagain/sentiment-source-code}, many of which do not include
  inner-word punctuation in their \emph{definition} of words, resulting in
  less than perfect results\footnote{In fact, all found implementations
      deploy lacking tokenisations steps.
    Dubious, as they each create unreachable code through their na\"ivety:
      all implementations remove dashes from words, while words such as
      ``self-deluded'' are included in the databases they use, but never
      reachable.}.

\subsubsection{Automatic Summarisation}\label{automatic-summarisation}

Automatic summarisation is an \gls{nlp} task concerned with the reduction
  of text to the \emph{major} points retaining the original document.
Few open source implementations of automatic summarisation algorithms on
  the web, in contrast with implementations for sentimental analysis, were
  found\footnote{For example, on the web only \emph{node-summary} was found
    \autocite{jbrooksuk/node-summary-source-code}, in Scala \emph{textteaser}
    was found \autocite{MojoJolo/textteaser-source-code}.}.

Automatic summarisation could be implemented as follows:

\begin{enumerate}
\item\emph{Detect Language (optional)};
\item\emph{Sentence Tokenisation (optional)} --- Unless even finer grained
  control over the document is possible (tokenising phrases), sentences are
  the smallest unit that should stay intact in the resulting summary;
\item\emph{Word Tokenisation} --- Needed to calculate keywords (words
  which occur more often than expected by chance alone);
\item\emph{Automatic summarisation}.
\end{enumerate}

\noindent Automatic summarisers typically return the highest ranking units,
  be it sentences or phrases, according to several factors:

\begin{aenumerate}
\item\emph{Number of words} --- An ideal sentence is neither too long nor
  too short;
\item\emph{Number of keywords} --- Words which occur more often than
  expected by chance alone in the text;
\item\emph{Similarity to title} --- Number of words from the document's
  title the unit contains;
\item\emph{Position inside parent} --- Initial and final sentences of a
  paragraph are often more important than sentences buried somewhere in
  the middle.
\end{aenumerate}

\noindent Some implementations include only keyword metrics
  \autocite{jbrooksuk/node-summary-source-code}, others include all features
  \autocite{MojoJolo/textteaser-source-code}, or even more advanced factors
  \autocite{summly}.

The only implementation working on the web, by James Brook
  \autocite*{jbrooksuk/node-summary-source-code}, takes a na\"ive sentence
  tokenisation approach.
Such as ignoring sentences terminated by exclamation marks.
Both other implementations, and many more, use a different approach to
  sentence tokenisation: Corpora.

\subsection{Using Corpora for \textsc{nlp}}\label{using-corpora-for}

A corpus is a large, structured set of texts used for many \gls{nlp}
  and linguistics tasks.
Corpora contain items (often words, but sometimes other units) annotated
  with information (such as \gls{pos} tags or lemmas).

These colossal (often more than a million words\footnote{The Brown Corpus
    contains about a million words \autocite{francis-nelson-brown-corpus},
    the Google N-Gram Corpus contains 155 billion
    \autocite{brants-thorsten-google-ngram-corpus}.})
  lumps of data are the basis of many of the newer revolutions in \gls{nlp}
  \autocite{mitkov-ruslan-ea-importance-corpora}.
Parsing based on supervised learning (in \gls{nlp}, based on annotated
  corpora), is the opposite of rule based parsing\footnote{A simple
    rule based sentence tokeniser could be implemented as follows
    \autocite{attivio.com-doing-things-with-sentences}:

    \begin{aenumerate}
      \item If it is a full stop, it ends a sentence;
      \item If the full stop is preceded by an abbreviation, it does not end
        a sentence;
      \item If the next token is capitalised, it ends a sentence.
    \end{aenumerate}}.
Instead of rules (and exceptions to these rules, exceptions to these
  exceptions, and so on) specified by a developer,
  supervised learning\footnote{``{[}From{]} a set of labelled examples as
    training data {[}, make{]} predictions for all unseen points''
    \autocite{mohri-mehryar-foundations-machine-learning}.}
  delegates this task to machines.
This delegation results in a more efficient, scalable, program.

Parsing based on corpora has proven better in several ways over rule based
  parsing, but has disadvantages:

\begin{enumerate}
\item Good training sets are required;
\item If the corpus was created from news articles, algorithms based on it
  will not fare so well on microblogs (such as Twitter posts).
\item Some rule based approaches for pre- and post processing are still
  required.
\end{enumerate}

\noindent In addition, corpora based parsing will not work well on the web.
Loading corpora over the network each time a user requests a web page is
  infeasible for most websites and applications\footnote{Currently,
    one technology exists for storing large datasets in a browser: the
    \acrshort{html5} File System \acrshort{api}. However, ``work on this
    document has been discontinued'', and the specification ``should not be
    used as a basis for implementation'' \autocite{urhane-file-api}.}.

Two viable alternative approaches exist for the web: rule based tokenisation,
  or connecting to a server over the network.

\subsection{Using a web \textsc{api}}\label{using-a-web}

Where the term \gls{api} stands for an interface between two programs,
  it is often used in web development as requests (from a web browser),
  and responses (from a web server) over \gls{http}.
For example, Twitter has such a service to allow developers to list,
  replace, create, and delete so-called tweets and other objects (users,
  images, \&c.).
This paper uses the term Web \gls{api} for the latter, and \gls{api} for
any programming interface.

With the rise of the asynchronous web\footnote{Starting around 2000,
    \gls{ajax} started to transform the web.
  Beforehand, significant changes to websites only occurred when a user
    navigated to a new page. 
  With \gls{ajax} however, new content arrived to users without the need
    for a full page refresh. One of the first examples are Outlook Web
    Access in 2000 \autocite{technet-outlook-web-access} and Gmail in 2004
    \autocite{gmailblog-gmail-ajax}, both examples of how \gls{ajax} made
    the web feel more like native applications.},
  supervised learning became available through web \glspl{api}
  \autocites{textteaser-web-api}{wordnet-web-api}{textrazor-web-api}.
This made it possible to use supervised learning techniques on the web,
  without needing to download corpora to a users' computer.

However, accessing \gls{nlp} web \glspl{api} over a network has
  disadvantages.
Foremost of which the time involved in sending data over a network and
  bandwidth used (especially on mobile networks), and heightened security
  risks.
