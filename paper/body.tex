\chapter{Context}\label{context}

% Reset glossary expansions.
\glsresetall

\section{Natural Language Processing}\label{natural-language-processing}

\begin{quote}
  \textit{Natural Language Processing is a theoretically motivated range
    of computational techniques for analyzing and representing
    naturally occurring texts at one or more levels of linguistic
    analysis for the purpose of achieving human-like language processing
    for a range of tasks or applications.
  }

  \medskip
  --- Elizabeth D. Liddy \autocite*{natural-language-processing-liddy-2001}
\end{quote}

\noindent The focus of this paper is \gls{nlp}.
\gls{nlp} is a field related to human--computer interaction, as it
  concerns itself with enabling machines to understand human language.
Human language, a medium which is easy for humans to understand, poses
  problems for machines.
The Georgetown--\gls{ibm} experiment in 1945, one of the first application
  of \gls{nlp}, illustrates this difficulty.
During this study in New York, scientists demonstrated a
  Russian--English translation system
  \autocite{hutchins-john-georgetown-ibm-system}.
The machine translated more than sixty sentences from Russian to English.
The experiment was well publicised and resulted in optimism.
The public believed machine translation would be a ``solved problem'' within
  three to five years.
Despite promising first results, the following ten years were disappointing
  and led to reduced funding.

Machine translation is just one of many major tasks involved with \gls{nlp}.
Other tasks include generating summaries, detecting references to people
  and places, or extracting opinion.
Many programs exist to carry out these and many other \gls{nlp} tasks.
The approach taken to perform these tasks are often similar between
  implementations.
For example, Entity linking is often implemented as follows
  \autocite[according to][]{stanbol-enhancer-nlp}:

\begin{enumerate}
\item\emph{Language Detection (optional)} --- Based on the language of the
    given text, the algorithms behind the following steps will change.
  Omitted if the implementation supports a single language;
\item\emph{Sentence Tokenisation (optional)} --- Sentence breaking elevates
    performance and heightens accuracy of the following stages, in
    particular \acrshort{pos} tagging;
\item\emph{Word Tokenisation} --- The entities (words) must be free from
  their surroundings;
\item\emph{\acrfull{pos} Tagging (optional)} --- It is often desired to link
    several nouns or proper nouns.
  Detecting word categories makes this achievable;
\item\emph{Noun Phrase Detection (optional)} --- Although \emph{apple} and
    \emph{juice} could be two entities, it is more appropriate to link to
    one entity: \emph{apple juice}.
  Detecting noun phrases makes this possible;
\item\emph{Lemmatisation or Stemming (optional)} --- Be it \emph{walk},
    \emph{walked}, or \emph{walking}, all forms of \emph{walk} could link
    to the same entity.
  Detecting either makes this possible;
\item\emph{Entity Linking} --- Linking detected entities to references, such
    as an encyclopaedia.
\end{enumerate}

\noindent \gls{nlp} covers many tasks, but the process of accomplishing these
  goals touches, as seen above, on well-defined stages.

\section{Scope}\label{scope}

Although many \gls{nlp} tasks exist, the standard and the implementation
  this paper proposes will only cover one: \emph{tokenisation}.
Tokenisation, as defined here, includes breaking sentences, words, and
other grammatical units.

Another confinement set to scope the proposal, is that it focusses on
  syntactic grammatical units. Thus, semantic units (i.e., phrases and
  clauses) are ignored.

In addition, the paper focusses on written language (text), thus ignoring
  spoken language.

Last, this paper focusses on Latin script languages: written languages
  using an alphabet based on the classical Latin alphabet.

\section{Implementations}\label{implementations}

While researching algorithms to tokenise natural language few viable
  implementations were found.
Most algorithms look at either sentence- or word tokenisation (rarely both).
This section describes the current implementations, where they excel, and
  what they lack.

\subsection{Stages}\label{stages}

This section delves into how current implementations accomplish
  tokenisation tasks.

\subsubsection{Sentence tokenisation}\label{sentence-tokenisation}

Often referred to as sentence boundary disambiguation\footnote{Both
    sentence tokenisation and sentence boundary disambiguation detect
      sentences.
    Sentence boundary disambiguation focusses on the position where
      sentences break (as in, ``One sentence?\textbar{} Two
      sentences.\textbar{}'', where the pipe symbols refer to the end of
      one sentence and the beginning of another). Sentence
      tokenisation targets both the start and end positions (as in,
      ``\{One sentence?\} \{Two sentences.\}'', where everything between
      braces is classified as a sentence).}, sentence
  tokenisation is an elementary but important part of \gls{nlp}.
It is almost always a stage in \gls{nlp} applications and not an end goal.
Sentence tokenisation makes other stages (e.g., detecting plagiarism or
  \gls{pos} tagging) perform better.

Often, sentences end in one of three symbols: either a full stop (.),
  an interrogative point (?), or an exclamation point (!)\footnote{One
    could argue the in 1962 introduced
    obscure interrobang (â€½), used to punctuate rhetorical statements where
    neither the question nor exclamation alone exactly serve the writer
    well, should be in this list \autocite{interrobang-mks.com}.}.
But detecting the boundary of a sentence is not as simple as breaking it at
  these markers: they might serve other purposes.
Full stops often occur in numbers, suffixed to abbreviations or titles,
  in initialisms\footnote{Although
    the definition of initialism is ambiguous, this paper defines its use
    as an acronym (an abbreviation formed from initial components, such as
    ``sonar'' or ``\textsc{fbi}'') with full stops depicting elision (such as
    ``e.g.'', or ``K.G.B.'').},
  or in embedded content\footnote{Embedded
    content in this paper refers to an external (ungrammatical) value
    embedded into a grammatical unit, such as a \textsc{url} or an emoticon.
    Note that these embedded values often consist of valid words and
    punctuation marks, but almost always should not be classified as such.}.
The interrogative- and exclamation points too can occur ambiguously, such as
  in a quote (e.g., `\,``Of course!'', she screamed').

Disambiguation gets even harder when these exceptions \emph{are} in fact a
  sentence boundary (double negative), such as in
  ``\ldots{}use the feminine form of idem, ead.'' or in
  `\,``Of course!'', she screamed, ``I'll do it!''\,', where in both
  cases the last terminal marker ends the sentence.

\subsubsection{Word tokenisation}\label{word-tokenisation}

Like sentence tokenisation, word tokenisation is another elementary but
important stage in \gls{nlp} applications. Whether stemming, finding
phonetics, or \gls{pos} tagging, tokenising words is an important
precursory step.

Often implementations see words as everything that is \emph{not} white
  space (i.e., spaces, tabs, feeds) and their boundaries as everything that
  is \autocite{loadfive/knwl-source-code}.

Some implementations take punctuation marks into account as boundaries.
This practice has flaws, as it results in the faulty classification of
  inner-word punctuation\footnote{Many such inner-word symbols exist, such
    as hyphenation points, colons (``12:00''), or elision (whether denoted
    by full stops, ``e.g.''; apostrophes, the Dutch ``'s''; or slashes,
    ``N/A'').}
  as part of the surrounding word \autocite{NaturalNode/natural-source-code}.

\subsection{Tasks}\label{tasks}

The previous section covered implementations that solve tokenisation stages
  in \gls{nlp} applications, such as Natural's word
  tokenisers \autocite{NaturalNode/natural-source-code}.
Concluded was that these implementations are lacking.
This section covers several implementations that solve these stages
  as part of a larger task.

\subsubsection{Sentiment Analysis}\label{sentiment-analysis}

Sentiment analysis is an \gls{nlp} task concerned with the polarity
  (positive, negative) and subjectivity (objective, subjective) of text.
It could be part of an implementation to detect messages with a certain
  polarity.
Twitter allows its users to search on polarity.
For example, when a user searches for ``movie :)'', Twitter searches for
  positive tweets.

Sentiment analysis could be implemented as follows:

\begin{enumerate}
\item\emph{Detect Language (optional)};
\item\emph{Sentence Tokenisation (optional)} --- Different sentences have
    different sentiments, tokenising them helps provide better results;
\item\emph{Word Tokenisation} --- Needed to compare with the database;
\item\emph{Lemmatisation or Stemming (optional)} --- Helps classification;
\item\emph{Sentiment Analysis}.
\end{enumerate}

\noindent Sentiment analysers typically include a
  database mapping either words, stems, or lemmas to their respective
  polarity and\slash or subjectivity\footnote{For example, the \textsc{afinn}
    database mapping words to polarity \autocite{nielsen-finn-arup-afinn}.}
  and return the average sentiment per sentence, or for the document.

In the case of the previously mentioned Twitter example, the service filters
  out all neutral and negative results, and return the remaining (positive
  attitude) results.

Many implementations exist for this task
  \autocites{thinkroth/sentimental-source-code}{mileszim/sediment-source-code}
  {thisandagain/sentiment-source-code}, many of which do not include
  inner-word punctuation in their \emph{definition} of words, resulting in
  less than perfect results\footnote{In fact, all found implementations
      deploy lacking tokenisations steps.
    Dubious, as they each create unreachable code through their na\"ivety:
      all implementations remove dashes from words, while words such as
      ``self-deluded'' are included in the databases they use, but never
      reachable.}.

\subsubsection{Automatic Summarisation}\label{automatic-summarisation}

Automatic summarisation is an \gls{nlp} task concerned with the reduction
  of text to the \emph{major} points retaining the original document.
Few open source implementations of automatic summarisation algorithms on
  the web, in contrast with implementations for sentimental analysis, were
  found\footnote{For example, on the web only \emph{node-summary} was found
    \autocite{jbrooksuk/node-summary-source-code}, in Scala \emph{textteaser}
    was found \autocite{MojoJolo/textteaser-source-code}.}.

Automatic summarisation could be implemented as follows:

\begin{enumerate}
\item\emph{Detect Language (optional)};
\item\emph{Sentence Tokenisation (optional)} --- Unless even finer grained
  control over the document is possible (tokenising phrases), sentences are
  the smallest unit that should stay intact in the resulting summary;
\item\emph{Word Tokenisation} --- Needed to calculate keywords (words
  which occur more often than expected by chance alone);
\item\emph{Automatic summarisation}.
\end{enumerate}

\noindent Automatic summarisers typically return the highest ranking units,
  be it sentences or phrases, according to several factors:

\begin{aenumerate}
\item\emph{Number of words} --- An ideal sentence is neither too long nor
  too short;
\item\emph{Number of keywords} --- Words which occur more often than
  expected by chance alone in the text;
\item\emph{Similarity to title} --- Number of words from the document's
  title the unit contains;
\item\emph{Position inside parent} --- Initial and final sentences of a
  paragraph are often more important than sentences buried somewhere in
  the middle.
\end{aenumerate}

\noindent Some implementations include only keyword metrics
  \autocite{jbrooksuk/node-summary-source-code}, others include all features
  \autocite{MojoJolo/textteaser-source-code}, or even more advanced factors
  \autocite{summly}.

The only implementation working on the web, by James Brook
  \autocite*{jbrooksuk/node-summary-source-code}, takes a na\"ive sentence
  tokenisation approach.
Such as ignoring sentences terminated by exclamation marks.
Both other implementations, and many more, use a different approach to
  sentence tokenisation: Corpora.

\subsection{Using Corpora for \textsc{nlp}}\label{using-corpora-for}

A corpus is a large, structured set of texts used for many \gls{nlp}
  and linguistics tasks.
Corpora contain items (often words, but sometimes other units) annotated
  with information (such as \gls{pos} tags or lemmas).

These colossal (often more than a million words\footnote{The Brown Corpus
    contains about a million words \autocite{francis-nelson-brown-corpus},
    the Google N-Gram Corpus contains 155 billion
    \autocite{brants-thorsten-google-ngram-corpus}.})
  lumps of data are the basis of many of the newer revolutions in \gls{nlp}
  \autocite{mitkov-ruslan-ea-importance-corpora}.
Parsing based on supervised learning (in \gls{nlp}, based on annotated
  corpora), is the opposite of rule based parsing\footnote{A simple
    rule based sentence tokeniser could be implemented as follows
    \autocite{attivio.com-doing-things-with-sentences}:

    \begin{aenumerate}
      \item If it is a full stop, it ends a sentence;
      \item If the full stop is preceded by an abbreviation, it does not end
        a sentence;
      \item If the next token is capitalised, it ends a sentence.
    \end{aenumerate}}.
Instead of rules (and exceptions to these rules, exceptions to these
  exceptions, and so on) specified by a developer,
  supervised learning\footnote{``{[}From{]} a set of labelled examples as
    training data {[}, make{]} predictions for all unseen points''
    \autocite{mohri-mehryar-foundations-machine-learning}.}
  delegates this task to machines.
This delegation results in a more efficient, scalable, program.

Parsing based on corpora has proven better in several ways over rule based
  parsing, but has disadvantages:

\begin{enumerate}
\item Good training sets are required;
\item If the corpus was created from news articles, algorithms based on it
  will not fare so well on microblogs (such as Twitter posts).
\item Some rule based approaches for pre- and post processing are still
  required.
\end{enumerate}

\noindent In addition, corpora based parsing will not work well on the web.
Loading corpora over the network each time a user requests a web page is
  infeasible for most websites and applications\footnote{Currently,
    one technology exists for storing large datasets in a browser: the
    \acrshort{html5} File System \acrshort{api}. However, ``work on this
    document has been discontinued'', and the specification ``should not be
    used as a basis for implementation'' \autocite{urhane-file-api}.}.

Two viable alternative approaches exist for the web: rule based tokenisation,
  or connecting to a server over the network.

\subsection{Using a web \textsc{api}}\label{using-a-web}

Where the term \gls{api} stands for an interface between two programs,
  it is often used in web development as requests (from a web browser),
  and responses (from a web server) over \gls{http}.
For example, Twitter has such a service to allow developers to list,
  replace, create, and delete so-called tweets and other objects (users,
  images, \&c.).
This paper uses the term Web \gls{api} for the latter, and \gls{api} for
any programming interface.

With the rise of the asynchronous web\footnote{Starting around 2000,
    \gls{ajax} started to transform the web.
  Beforehand, significant changes to websites only occurred when a user
    navigated to a new page. 
  With \gls{ajax} however, new content arrived to users without the need
    for a full page refresh. One of the first examples are Outlook Web
    Access in 2000 \autocite{technet-outlook-web-access} and Gmail in 2004
    \autocite{gmailblog-gmail-ajax}, both examples of how \gls{ajax} made
    the web feel more like native applications.},
  supervised learning became available through web \glspl{api}
  \autocites{textteaser-web-api}{wordnet-web-api}{textrazor-web-api}.
This made it possible to use supervised learning techniques on the web,
  without needing to download corpora to a users' computer.

However, accessing \gls{nlp} web \glspl{api} over a network has
  disadvantages.
Foremost of which the time involved in sending data over a network and
  bandwidth used (especially on mobile networks), and heightened security
  risks.

\chapter{Research Framework}\label{research-framework}

The objective of this paper is stated (Â§~\ref{research-objective}).
From the objective a research question is drafted
  (Â§~\ref{research-question}).
Additionally, from the research question, several research questions are
  drafted, acting as a guideline for the proposal
  (Â§~\ref{research-questions}).

\section{Research Objective}\label{research-objective}

Using the context described in chapter \ref{context} (p.~\pageref{context}),
  the objective of this research project is constructed:

\begin{quote}
  \textit{A generic documentation (the ``specification'') and example
    implementation (the ``program'') that exposes an interface (the
    ``\acrshort{api}'') for the topic (``text manipulation'') based on real
    use cases of potential users (the ``developer'') on the platform (the
    ``web'').
  }
\end{quote}

\section{Research Question}\label{research-question}

This research objective leads to a research question:

\begin{quote}
  \textit{How can a specification and program, that exposes an \acrshort{api}
    for text manipulation, based on use cases of developers on the web,
    be developed?
  }
\end{quote}

\section{Research Questions}\label{research-questions}

The previously defined research question is split into several smaller
  questions.
These research questions form a basis and a guide to answer the research
  question, to reach the research objective.

\begin{enumerate}
\item
  What are current possibilities and deficiencies in \gls{nlp}?
  \begin{enumerate}
    \item What current implementations exist?
    \item What does not yet exist?
  \end{enumerate}
\item
  What are the target audience's use cases for an implementation?
  \begin{enumerate}
    \item What would they use an implementation for?
    \item What would they \emph{not} use the implementation for?
  \end{enumerate}
\item
  How to ensure a quality implementation for the target audience?
  \begin{enumerate}
    \item What makes a good \gls{api} design?
    \item What makes a good implementation?
  \end{enumerate}
\end{enumerate}

\chapter{Production}\label{production}

\section{Target Audience}\label{target-audience}

The audience that benefits the most from the proposal, are web developers.
Web developers are programmers who specialise in creating software that
  functions on the world wide web.
A group which enables machines to respond to humans.
They engage in client side development (building the interface between
  a human and a machine on the web), and sometimes also in server side
  development (building the interface between the client side and a
  server).

Typical areas of work consist of programming in \gls{ecmascript},
  marking up documents in \gls{html}, graphic design through \gls{css},
  creating a back end in Node.js, \gls{php}, or other platforms, contacting a
  \gls{mongodb}, \gls{mysql}, or other database, and more.

Additionally, many interdisciplinary skills, such as usability,
  accessibility, copywriting, information architecture, or optimisation,
  are also of concern to web developers.

\section{Use cases}\label{use-cases}

The use cases of the target audience, the web developer, in the field of
  \gls{nlp} are many.
Research for this paper found several use cases, although it is
  expected many more could be defined.
The tasks below are each categorised into broad, generic fields: analysation,
  manipulation, and creation.

\begin{aenumerate}
\item\label{list:use-case:1} The developer may intent to summarise natural
  text (mostly analysation, potentially also manipulation);
\item\label{list:use-case:2} The developer may intent to create natural
  language, e.g., displaying the number of unread messages: ``You have 1
  unread message,'' or ``You have 0 unread messages'' (creation);
\item\label{list:use-case:3} The developer may intent to recognise sentiment
  in text: is a \emph{tweet} positive, negative, or spam? (analysation);
\item\label{list:use-case:4} The developer may intent to replace so-called
  \emph{dumb} punctuation with \emph{smart} punctuation, such as dumb
  quotations with (``) or (''), three dots with an ellipsis (\ldots{}), or
  two hyphens with an en-dash (--) (manipulation);
\item\label{list:use-case:5} The developer may intent to count the number of
  certain grammatical units in a document, such as, words, white space,
  punctuation, sentences, or paragraphs (analysation);
\item\label{list:use-case:6} The developer may intent to recognise the
  language in which a document is written (analysation);
\item\label{list:use-case:7} The developer may intent to find words in a
  document based on a search term, with regards for the lemma (or stem)
  and\slash or phonetics (so that a search for ``smit'' also returns similar
  words, such as ``Schmidt'' or ``Smith'') (analysation and manipulation).
\end{aenumerate}

\noindent \gls{nlp} is a large field with many challenges, but not every
  challenge in the field is of interest to the web developer.
Foremost, the more academic areas of \gls{nlp}, such as speech recognition,
  optical character recognition, text to speech transformation, translation,
  and machine learning, do not fit well with the goals of web developers.

\section{Requirements}\label{requirements}

The proposal must enable the target audience to reach the in the previous
  section defined use cases.
In addition, the proposal should meet several other requirements to better
  suit the wishes of the target audience.

\subsection{Open Source}\label{open-source}

To reach the target audience and validate its usability, the proposal
  should be open source.
All code should be licensed under \acrshort{mit}, a license which
  provides rights for others to use, copy, modify, merge, publish,
  distribute, sublicense, and\slash or sell copies of the code it covers.

In addition, the software should be developed under the all-seeing eye of
  the community: on GitHub.
GitHub is a hosted version control\footnote{Version
    control services manage revisions to documents, popularly used for
    controlling and tracking changes in software.} service with social
  networking features.
On GitHub, web developers follow their peers to track what they are
  working on, watch their favourite projects to get notified of changes,
  and raise issues and request features.

\subsection{Performance}\label{performance}

The proposal should execute at high \emph{performance}.
Performance includes the software having a small file size to reach the
  client over the network with the highest possible speed, but most
  importantly that the execution of code should run efficiently and at high
  speeds.

\subsection{Testing}\label{testing}

\emph{Testing} should have high priority in the proposal.
Testing, in software development, refers to validating if software does what
  it is supposed to do, and can be divided into several subgroups:

\begin{aenumerate}
\item\emph{Unit testing} --- Validation of each specific section of code;
\item\emph{Integration testing} --- Validation of how programs work together;
\item\emph{System testing} --- Validation of if the system meets its
  requirements;
\item\emph{Acceptance testing} --- Validation of the end product.
\end{aenumerate}

\noindent Great care should be given to develop an adequate test suite with
  full \emph{coverage} for every program.
Coverage, in software development, is a term used to describe the amount of
  code tested by the test suite.
Full coverage means every part of the code is reached by the tests.

Unit test run through Mocha \autocite{visionmedia/mocha-source-code},
  coverage is detected by Istanbul
  \autocite{gotwarlost/istanbul-source-code}.

\subsection{Code quality}\label{code-quality}

\emph{Code quality}---how useful and readable for both humans and machines
  the software is---should be vital. For humans, the code should be
  consistent and clear. For computers, the code should be free of bugs and
  other suspicious code.

\subsubsection{Suspicious Code and Bugs}\label{suspicious-code-and-bugs}

To detect bugs and suspicious code in the software, \emph{Eslint}
  is used \autocite{eslint/eslint-source-code}.
\emph{Linting}, in computer programming, is a term used to describe static
  code analysis to detect syntactic discrepancies without running the code.
\emph{Eslint} is used because it provides a solid set of basic rules and
  enables developers to create custom rules.

\subsubsection{Style}\label{style}

To enforce a consistent code style---to create readable software for
  humans---\acrshort{jscs} is used \autocite{mdevils/node-jscs-source-code}.
\acrshort{jscs} provides rules for (dis)allowing certain code patterns,
  such as white space at the end of a line or camel cased variable names,
  or enforcing a maximum line length.
\acrshort{jscs} was chosen because it, like \emph{Eslint}, provides a strong
  basic set of rules.
The rules chosen for the proposal were set strict to enforce all code to be
  written in the same way.

\subsubsection{Commenting}\label{commenting}

Even when code is bug free, uses no confusing short-cuts, and adheres to a
  strict style, it might still be hard to understand for humans.
\emph{Commenting} code---describing what a program does and why it
  accomplishes it this way---is important.
However, commenting can also be too verbose, such as when the code is
  duplicated in natural language.

\gls{jsdoc} \autocite{google.com-clojure-compiler-jsdoc} is a markup language
  for \gls{ecmascript} that allows developers to embed documentation---using
  comments---in source code.
Several tools can later extract this information and expose it independent
  from the original code.
``Tricky'' code should be annotated inside the software with comments to help
  readers understand why certain decisions were made.

\subsection{Automation}\label{automation}

When suspicious, ambiguous, or buggy code is introduced in the software, the
  error should be automatically detected.
Sometimes, deployment should be prevented.
Automated \gls{ci} environments to enforce error detection should be used.
To detect complex, duplicate, or bug prone code, Code Climate is used
  \autocite{codeclimate.com}.
To validate all tests passed before deploying the software, Travis is used
  \autocite{travis-ci.org}.

\subsection{\textsc{api} Design}\label{design-1}

Quality interface design should have high priority for the proposal.
A good \gls{api}, according to Joshua Bloch
  \autocite*{bloch-joshua-how-design-good-api-why-matters}, has the following
  characteristics:

\begin{enumerate}
\item Easy to learn;
\item Easy to use;
\item Hard to misuse;
\item Easy to read;
\item Easy to maintain;
\item Easy to extend;
\item Meeting its requirements;
\item Appropriate for the target audience.
\end{enumerate}

\noindent In essence equal, but worded differently, are the characteristics
  of good \gls{api} design according to the Qt Project
  \autocite*{qt-project.org-api-design-principles}:

\begin{enumerate}
\item Be minimal;
\item Be complete;
\item Have clear and simple semantics;
\item Be intuitive;
\item Be easy to memorise;
\item Lead to readable code.
\end{enumerate}

\noindent The proposal should take these characteristics, and the in their
  sources given examples, into account.

\subsection{Installation}\label{installation}

Simple access to the software for the target audience, both on the client
  side and on the server side, should be given high priority.
On the client side, many package managers exist, the most
  popular being Bower and Component\footnote{Popularity here is simply
    defined as having the highest number of search results on Google.}.
For Node.js (on the server side), \gls{npm} is the most popular.
To reach the target audience, besides making the source available on GitHub,
  all popular package managers, \gls{npm}, Bower, and Component, are used.

\chapter{Design \& Architecture}\label{design}

The in this paper presented solution to the problem of \gls{nlp} on the
client side is split up in multiple small proposals. Each sub-proposal
solves a sub-problem.

\begin{aenumerate}
\item\emph{\acrshort{nlcst}} --- Defines a standard for classifying
  grammatical units understandable for machines;
\item\emph{parse-latin} --- Classifies natural language according to
  \gls{nlcst};
\item\emph{\gls{textom}} --- Provides an interface for analysing and
  manipulating output provided by parse-latin;
\item\emph{Retext} --- Provides an interface for transforming natural
  language into an object model and exposes an interface for plug-ins.
\end{aenumerate}

\noindent The decoupled approach taken by the provided solution enables other
  developers to implement their own software to replace sub-proposals.
For example, other parties could create a parser for the Chinese language and
  use it instead of \emph{parse-latin} to classify natural language according
  to \gls{nlcst}, or other parties can implement an interface like
  \gls{textom} with functionality for phrases and clauses.

\section{Syntax: \textsc{nlcst}}\label{syntax}

To develop natural language tools in \gls{ecmascript}, an intermediate
  representation of natural language is useful.
Instead of each module (such as, every stage in section
  \ref{sentiment-analysis} on page~\pageref{apendix-a-nlcst-definition})
  defining its own representation of text, using a single syntax leads to
  better interoperability, performance, and results.

The elements defined by \acrfull{nlcst} are based on the grammatical
  hierarchy, but by default do not expose all its constituents\footnote{The
      grammatical hierarchy of text is constituted by words, phrases,
      clauses, and sentences.
    \glspl{nlcst} only implements the sentence and word constituents
    by default, although clauses and phrases could be provided by
    implementations.}.
Additionally, \gls{nlcst} provides more elements to cover other semantic
  units in natural language\footnote{Most
    notably, punctuation, symbol, and white space elements.}.

The definitions were influenced by other syntax trees specifications for
  manipulation on the web platform, such as \emph{\textsc{css}}, eponymous
  for the \acrshort{css} language \autocite{reworkcss/css-source-code} or the
  \emph{Mozilla JavaScript \textsc{ast}}, for \gls{ecmascript}
  \autocite{mozilla.org-spidermonkey-parser_api}.

Both widely used implementations, \emph{\textsc{css}} by Rework
  \autocite{reworkcss/rework-source-code}, and \emph{Mozilla JavaScript
  \textsc{ast}} by Esprima \autocite{ariya/esprima-source-code}, Acorn
  \autocite{marijnh/acorn-source-code}, and Escodegen
  \autocite{constellation/escodegen-source-code}.

\gls{nlcst} is different from both these specifications, as it implements a
  \gls{cst}, where the others use an \gls{ast}.
A \gls{cst} is a one-to-one mapping of source (such as natural language)
  to result (a tree).
All information stored in the source is also available through the tree
  \autocite{thegreenplace.net-abstract-concrete-syntax-trees}.
This makes it easy for developers to save the output or pass it on to other
  libraries for further processing.
However, the information stored in \glspl{cst} is verbose, which might be
  difficult to work with.

\medskip \noindent See appendix~\ref{apendix-a-nlcst-definition} on
page~\pageref{apendix-a-nlcst-definition} for a complete list of
specified nodes of \gls{nlcst}.

\section{Parser: parse-latin}\label{parser-parse-latin}

To create a syntax tree according to \gls{nlcst} from natural language,
  this paper presents \emph{parse-latin} for Latin script based
  languages\footnote{Such
    as Old-English, Icelandic, French, or even scripts slightly similar, such
    as Cyrillic, Georgian, or Armenian.}.
Additionally, to prove the concept, two other libraries are presented,
  \emph{parse-english} and \emph{parse-dutch}. Both with \emph{parse-latin}
  as a basis, but providing better support for several language specific
  features, respectively for English and Dutch.

By using the \gls{cst} as described by \gls{nlcst} and the \emph{parse-latin}
  parser, the intermediate representation can be used by developers to
  create independent modules which may receive better results or
  performance over implementing their own parsing tools.

In essence, \emph{parse-latin} tokenises text into white space, word, and
  punctuation tokens.
\emph{parse-latin} starts out with a pretty simple definition, one that
  some other tokenisers also implement \autocite{treebank-tokenisation}:

\begin{enumerate}
\item A \emph{word} is one or more letter or number characters;
\item A \emph{white space} is one or more white space characters;
\item A \emph{punctuation} is one or more of anything else.
\end{enumerate}

\noindent Then, \emph{parse-latin} manipulates and merges those tokens into a
  syntax tree, adding sentences, paragraphs, and other nodes where needed.
Most of the intellect of the algorithm deals with sentence tokenisation
  (Â§ \ref{sentence-tokenisation}, p. \pageref{sentence-tokenisation}).
This is done in similar fashion, but more intelligent, to \emph{Emphasis}
  \autocite{nytimes/emphasis-source-code}.

\begin{aenumerate}
\item\emph{Inter-word punctuation} --- Some punctuation marks are part of
  the word they occur in, such as the punctuation marks in ``non-profit'',
  ``she's'', ``G.I.'', ``11:00'', or ``N\slash A'';
\item\emph{Non-terminal full stops} --- Some full stops do not mark a
  sentence end, such as the full stops in ``1.'', ``e.g.'', or ``id.'';
\item\emph{Terminal punctuation} --- Although full stops, question marks,
  and exclamation marks (sometimes) end a sentence, that end might not occur
  directly after the mark, such as the punctuation marks after the full
  stop in ``.)'' or ``.'{}'';
\item\emph{Embedded content} --- Punctuation marks are sometimes used in
  non-standard ways, such as when a section or chapter delimiter is
  created with a line containing three asterisk marks (``* * *'').
\end{aenumerate}

\noindent See appendix~\ref{appendix-parse-latin} on
page~\pageref{appendix-parse-latin} for example output provided
by \emph{parse-latin}.

\subsection{parse-english}\label{parse-english}

\emph{parse-english} provides the same interface as \emph{parse-latin}, but
returns results better suited for English text.
Exceptions in the English language include:

\begin{aenumerate}
\item \emph{Unit abbreviations} --- ``tsp.'', ``tbsp.'', ``oz.'', ``ft.'',
  \&c.;
\item\emph{Time references} --- ``sec.'', ``min.'', ``tues.'', ``thu.'',
  ``feb.'', \&c.;
\item\emph{Business Abbreviations} --- ``Inc.'' and ``Ltd.''
\item\emph{Social titles} --- ``Mr.'', ``Mmes.'', ``Sr.'', \&c.;
\item\emph{Rank and academic titles} --- ``Dr.'', ``Gen.'', ``Prof.'',
  ``Pres.'', \&c.;
\item\emph{Geographical abbreviations} --- ``Ave.'', ``Blvd.'', ``Ft.'',
  ``Hwy.'', \&c.;
\item\emph{American state abbreviations} --- ``Ala.'', ``Minn.'', ``La.'',
  ``Tex.'', \&c.;
\item\emph{Canadian province abbreviations} --- ``Alta.'', ``QuÃ©.'',
  ``Yuk.'', \&c.;
\item\emph{English county abbreviations} --- ``Beds.'', ``Leics.'',
  ``Shrops.'', \&c.;
\item\emph{Elision (omission of letters)} --- ``'n'\,'', ``'o'', ``'em'',
  ``'twas'', ``'80s'', \&c.
\end{aenumerate}

\subsection{parse-dutch}\label{parse-dutch}

\emph{parse-dutch} has, like \emph{parse-english}, the same interface as
\emph{parse-latin}, but returns results better suited for Dutch text.
Exceptions in the Dutch language include:

\begin{aenumerate}
\item\emph{Unit and time abbreviations} --- ``gr.'', ``sec.'', ``min.'', ``ma.'',
  ``vr.'', ``vrij.'', ``febr'', ``mrt'', \&c.;
\item\emph{Many other common abbreviations} --- ``Mr.'', ``Mv.'', ``Sr.'',
  ``Em.'', ``bijv.'', ``zgn.'', ``amb.'', \&c.;
\item\emph{Elision (omission of letters)} --- ``d'\,'', ``'n'', ``'ns'',
  ``'t'', ``'s'', ``'er'', ``'em'', ``'ie'', \&c.
\end{aenumerate}

\section{Object Model: TextOM}\label{object-model}

To modify an \gls{nlcst} tree in \gls{ecmascript}, whether created by
  \emph{parse-latin}, \emph{parse-english}, \emph{parse-dutch}, or other
  parsers, this paper presents \gls{textom}.
\gls{textom} implements the nodes defined by \gls{nlcst},
  but provides an object-oriented style\footnote{Object-oriented
    programming is a style of programming, where classes, instances,
    attributes, and methods are important.} to manipulate these nodes.
\gls{textom} was designed in similarity to the \gls{dom}\footnote{See
    appendix~\ref{appendix-dom} on page~\pageref{appendix-dom} for more
    information on the \gls{dom}.},
  the mechanism used by browsers to expose \gls{html} through
  \gls{ecmascript} to developers.
Because of \glspl{textom} likeness to the \gls{dom}, \gls{textom} is
  easy to learn and familiar to the target audience.

\gls{textom} provides functionality for events (a mechanism for detecting
  changes), modification (inserting, removing, and replacing children
  into\slash from parents), and traversal (such as finding all words in a
  sentence).

\gls{nlcst} allows authors to extend the specification by defining their
  own units, such as creating phrase or clause nodes.
\gls{textom} allows for the same extension, and is built to work well
  with these ``unknown'' nodes.

\medskip \noindent See appendix~\ref{appendix-textom} on
  page~\pageref{appendix-textom} for the implementation details of
  \gls{textom}.

\section{Natural Language System:
  Retext}\label{natural-language-system-retext}

For natural language processing on the client side, this paper presents
  \emph{Retext}.
\emph{Retext} combines a parser, such as \emph{parse-latin} or
  \emph{parse-english}, with an object model: \gls{textom}.
Additionally, \emph{Retext} provides a minimalistic plug-in mechanism which
  enables developers to create and publish plug-ins for others to use, and in
  turn enables them to use others' plug-ins inside their projects.

\emph{Retext} provides a strong basis to use plug-ins to add simple natural
  language features to a website, but additionally provides functionality to
  extend this basis---create plug-ins, parsers, or other features---to
  create vast natural language systems.

\medskip \noindent See appendix~\ref{appendix-retext} on
  page~\pageref{appendix-retext} for a description of the interface
  provided by \emph{Retext} and example usage.

\chapter{Validation}\label{validation}

The presented proposal was validated through two approaches.
The design and the usability of the interface was validated through
  solving several use cases of the target audience with the proposal.
Interest in the proposal by the target audience was validated by
  measuring the enthusiasm showed by the open source community.

\section{Plugins}\label{plugins}

More than fifteen plug-ins for \emph{Retext} were created to confirm if,
  and validate how, the proposals integrated together, and how the system
  worked.

The proposal solves the creation of natural language by default (use case
  \ref{list:use-case:2}), but these plug-ins solve several others.
The developed plug-ins included implementations for:

\begin{aenumerate}
\item Transforming so-called dumb punctuation marks into more
  typographically correct punctuation marks, solving use case
  \ref{list:use-case:4} \autocite*{wooorm/retext-smartypants-source-code};
\item Transforming emoji short-codes (:cat:) into real emoji
  \autocite*{wooorm/retext-emoji-source-code};
\item Detecting the direction of text
  \autocite*{wooorm/retext-directionality-source-code};
\item Detecting phonetics
  \autocite*{wooorm/retext-double-metaphone-source-code};
\item Detecting the stem of words
  \autocite*{wooorm/retext-porter-stemmer-source-code};
\item Finding grammatical units, solving use case
  \ref{list:use-case:5}  \autocite*{wooorm/retext-visit-source-code};
\item Finding text, even misspelled, solving use case
  \ref{list:use-case:7} \autocite*{wooorm/retext-search-source-code};
\item Detecting \gls{pos} tags \autocite*{wooorm/retext-pos-source-code};
\item Finding keywords and -phrases
  \autocite*{wooorm/retext-keywords-source-code};
\item Detecting the language of text, solving use case \ref{list:use-case:6} 
  \autocite*{wooorm/retext-language-source-code}.
\end{aenumerate}

\noindent These plug-ins listed and the other plug-ins solve several use cases
  of the target audience (Â§~\ref{use-cases}).
The unsolved use cases can be solved using the plug-in mechanism provided by
  \emph{Retext}.
Summarising natural language (use case \ref{list:use-case:1}) is not yet
  solved, but can be by implementing the stages mentioned in
  Â§~\ref{automatic-summarisation} (p.~\pageref{automatic-summarisation}).
Neither is detecting sentiment (use case \ref{list:use-case:3}), but can be
  by implementing the stages mentioned in Â§~\ref{sentiment-analysis}
  (p.~\pageref{sentiment-analysis}).

During the development of these plug-ins, several problems were brought to
  light in the developed software.
These problems were recursively dealt with, back and forth, between the
  software and the plug-ins.
The software changed severely by these changes, which resulted in a
  better interface and usability.

\section{Reception}\label{reception}

To confirm interest by the target audience in the proposal, enthusiasm
  showed by the open source community was measured.
To initially spark interest, several websites and email newsletters were
  contacted to feature \emph{Retext}, either in the form of an article or as
  a simple link.
This resulted in coverage on high-profile websites
  \autocite{dailyjs.com-natural-language-parsing-retext} and newsletters
  \autocites{nodeweekly.com-47}{javascriptweekly.com-193}
  {newspaper.io/javascript-2014-08-11}.
Later, organic growth resulted in features on link roundups
  \autocites{github.com-awesome-machine-learning}{github.com-awesome-nodejs},
  Reddit \autocites{reddit.com-mention-1}{reddit.com-mention-2}
  {reddit.com-mention-3}.

In turn, these publications resulted in positive reactions, such as on
  Twitter \autocites{twitter.com-mention-1}{twitter.com-mention-2}
  {twitter.com-mention-3}{twitter.com-mention-4}{twitter.com-mention-5}
  {twitter.com-mention-6}, other websites, and both feedback and fixes
  on GitHub \autocites{github.com-issue-1}{github.com-issue-2}
  {github.com-issue-3}{github.com-pull-request}.

Additionally, many of the target audience started \emph{following} the
  project on GitHub \autocite{github.com-stargazers}.

\chapter{Conclusion}\label{conclusion}

This chapter consists of a short summary (Â§~\ref{summary}), a list
  limitations and suggestions for future work
  (Â§~\ref{limitations-future-work}), and a list of conclusions
  (Â§~\ref{conclusions}).

\section{Summary}\label{summary}

\gls{nlp} covers many tasks, but the process of accomplishing
  these goals touches on well-defined stages.
Such as \emph{tokenisation}, the focus of this paper.
Current implementations on the web platform are lacking.
In part, because techniques such as \emph{supervised learning} do not work
  on the web.

The audience that benefits the most from better parsing on the web platform,
  are web developers, a group which is more interested in practical use, and
  less so in theoretical applications.

The presented proposal is split up in several solutions: a specification, a
  parser, and an object model.
These solutions come together in a proposal: \emph{Retext}, a complete natural
  language system.
\emph{Retext} takes care of parsing natural language and enables users to
  create and use plug-ins.

The proposal was validated both by solving the audience's use cases, and
  by measuring the audience's enthusiasm.

\section{Limitations \& Future Work}\label{limitations-future-work}

The proposal leaves open many areas of interest for future investigation.
Some of which, are featured here.

\begin{aenumerate}
\item\emph{Internationalisation} --- Currently, the proposal is only
    tested on Latin script languages.
  The software was developed with other languages and scripts, such as
    Arabic, Hangul, Hebrew, and Kanji, in mind.
  Future work could expand support to include these scripts;
\item\emph{Difference application} --- Currently, the proposal does not
    support difference application.
  When a word is added at the end of a sentence, all steps to produce the
    output have to be revisited.
  Although the proposal is created with this in mind, no support has been
    added.
  Future work could include difference application support;
\item\emph{Non-rule based parsing} --- Currently, \gls{nlcst} trees are
    created with rule based parsers.
  But, corpora based parsers could also produce these trees.
  Future work could investigate and implement such supervised learning
    approaches.
\item\emph{Academic goals} --- Currently, the proposals cater to practical use
    cases.
  Future work could expand on this purview and implement more academic
    goals.
\item\emph{Semantic units} --- Currently, the proposals provide syntactic
    units.
  Future work could expand on this by providing information about phrases
    and clauses to users.
\item\emph{Import formants} --- Currently, the parsers each require plain
    text input.
  Future work could expand on this by allowing other input formats, such as
    MarkDown or \LaTeX.
\item\emph{Heighten Performance} --- The decision made to adopt an
    object-oriented approach for analysation and manipulation, came at a huge
    performance cut.
  When implementing both \gls{textom} and \emph{parse-latin} over just
    \emph{parse-latin}, performance decreases over 90\%.
  Future work should investigate and implement better performance.
\end{aenumerate}

\section{Conclusions}\label{conclusions}

This section evaluates if the research question(s) are answered, and if the
  research objective is reached.

\subsection{Current possibilities and
  deficiencies}\label{q-current-possibilities-and-deficientcies}

\gls{nlp} covers many tasks.
Within the scope of this thesis, only \emph{tokenisation} was covered
  (Â§ \ref{scope}, p. \pageref{scope}).
Most current implementations use tokenisation as part of a larger task
  (Â§ \ref{tasks}, p. \pageref{tasks}).
Implementations that provide tokenisation capabilities to other tasks,
  are lacking (Â§ \ref{stages}, p. \pageref{stages}).
It is concluded that quality implementations that offer tokenisation within
  the scope, do not exist.

\subsection{The target audience's use cases}\label{q-use-cases}

The audience that benefits the most from the proposal, are web developers
  (Â§ \ref{target-audience}, p. \pageref{target-audience}).
Research for this paper found that the target audience would use the
  implementation for several use cases.
Not every challenge in the field is of interest to the web developer.
In addition, concluded was that the more academic areas of \gls{nlp}, do not
  fit well with the goals of web developers (Â§ \ref{use-cases},
  p. \pageref{use-cases}).

\subsection{Quality implementation}\label{q-quality-implementation}

The proposal should meet several requirements, other than the use cases,
  to better suit the wishes of the target audience.
This includes open source development and easy installation, readable code,
  tested results, high performance, and a good interface design.
Concluded was that by following these best practises for code and creating
  an interface similar to for the target audience familiar projects, a
  ``good'' implementation was built (Â§ \ref{requirements},
  p. \pageref{requirements}).

\subsection{Research Question}\label{q-research-question}

The answers to the research questions, answers the complete
  \emph{research question}, within the scope (Â§ \ref{research-question},
  p. \pageref{research-question}).

\subsection{Research Objective}\label{q-research-objective}

Although \emph{how} was answered by the \emph{research question}, the
  working proposal reaches the research objective.
This was validated by the more than fifteen solved use cases
  (Â§ \ref{plugins}, p. \pageref{plugins}).

In addition to \emph{reaching} this objective, the measured enthusiasm
  showed by the target audience for the proposal confirmed the interest in
  such a proposal.
