\chapter{Context}\label{context}

\{\{Needs a nice quote about the definition of NLP\}\}

The focus of this paper is \gls{nlp}. \Gls{nlp} concerns itself with
enabling machines to understand human language, thus it is a field
related to human--computer interaction. Human language, a medium which
humans understand quite easily, can pose problems for machines.

That understanding of human language by machines is quite difficult, is
depicted by one of the first application of \gls{nlp}, during the
Georgetown--\acrshort{ibm} experiment, where a Russian-english machine
translation system was showcased in New York in 1945
\autocite{hutchins-john-georgetown-ibm-system}. More than sixty
sentences were translated by a machine from Russian to English. The
experiment was well publicised in the press and resulted in optimism for
machine-translation. Machine-translation was thought of as being a
solved problem within three to five years. The results in the following
ten years were however disappointing and eventually led to reduced
funding.

Machine translation is just one of many major tasks involved with
\gls{nlp}. Other applications include automatic summarisation
(generating a summary), named entity recognition (detecting references
to people or places), sentiment analysis (extracting opinion).

Summarisation, named entity recognition, and sentiment analyses are all
part of what is known as ``information extraction'', a term for the act
of finding certain data inside a natural language document. These, and
many other applications of \gls{nlp}, are implemented widely by many
programs. The approach taken for such a goal is mostly equivalent. For
example, the information extraction method for entity linking in a text
document could be as follows \autocite[according
to][]{stanbol-enhancer-nlp}:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Detect the input language --- \emph{Language Detection}: Based on the
  language of the given text, the algorithms behind the following steps
  will change significantly. Often omitted if only a single language is
  supported.
\item
  Optionally break sentences --- \emph{Sentence Tokenisation}: Breaking
  sentences can elevate performance and heighten the accuracy of the
  following stages (in particular \gls{pos} tagging);
\item
  Breaking words --- \emph{Word Tokenisation}: To detect entities, one
  must of course break those entities (words) free from their
  surroundings;
\item
  Detecting word-categories --- \emph{\acrfull{pos} Tagging}: When
  linking entities in an encyclopaedia, one would typically not want to
  link every entity to another entry, but most often only nouns or even
  proper nouns. \gls{pos} tagging helps make that decision;
\item
  Optionally detecting noun phrases --- \emph{Noun Phrase Detection}:
  Although ``apple'' and ``juice'' are both words and could both be
  linked to separate entities, it would be more apt to link both words
  to one entity: ``apple juice'', noun phrase detection makes this
  possible;
\item
  Optionally detecting lemmas or stems --- \emph{Lemmatisation or
  Stemming}: All forms of walk could link to the same entity, be it
  ``walk'', ``walked'', or ``walking''. Detecting the lemma or stem for
  a word makes this possible;
\item
  Linking the entities to their reference --- \emph{Entity Linking}.
\end{itemize}

Although many different fields are covered by \gls{nlp}, the process of
reaching those goals touches on some well defined stages as seen above.

\section{Scope}\label{scope}

The stages mentioned in the previous section are implemented in many
\gls{nlp} applications. This paper (the captured use cases and
requirements, the proposed standard, and the example implementation)
however will only cover one stage: tokenisation. Tokenisation here
includes sentence boundary detection, word tokenisation, and
tokenisation of other grammatical units.

Another decision made in scoping the \gls{nlp} problem is that that this
paper lays its focus on syntactic, and largely ignores semantic units
(i.e., phrases and clauses).

Lastly, this paper ignores spoken language, and lays its focus on Latin
script language: written languages using an alphabet depending on the
letters of the classical Latin alphabet.

\section{Implementations}\label{implementations}

While researching algorithms to tokenise natural language few viable
options were found. Most algorithms look at one or two (or both) kinds
of tokenisation: sentence tokenisation and word tokenisation. In this
section the current implementations, where they excel, and what they
lack.

\subsection{Stages}\label{stages}

\subsubsection{Sentence tokenisation}\label{sentence-tokenisation}

Often referred to as sentence boundary disambiguation\footnote{Both
  sentence tokenisation and sentence boundary solve a similar
  issue---detecting where sentences terminate. Sentence boundary
  disambiguation focusses on the position where a sentence breaks (as
  in, `One sentence.\textbar{} Two sentences.\textbar{}', where the pipe
  symbol refers to the end of one sentence and the beginning of
  another), whereas sentence tokenisation lays focus on where a sentence
  starts and where it ends (as in, `\{One sentence.\} \{Two
  sentences.\}', where everything between curly brackets, both start and
  end boundaries, are detected and classified as s sentence).}, sentence
tokenisation is a very basic, but important part of \gls{nlp}. It is
almost always a stage in an \gls{nlp} application and not an end goal.
Sentence tokenisation helps subsequent stages (e.g., detecting
plagiarism or \gls{pos} tagging) work more accurately.

Oftentimes, one of three symbols is used to denote the end of a
sentences: Either a full stop (.), an interrogative point (?), or an
exclamation point (!)\footnote{One could argue the in 1962 introduced
  obscure interrobang (‽), used to punctuate rhetorical statements where
  neither the question nor an exclamation alone exactly serve the writer
  well, should be in this list \autocite{interrobang-mks.com}.}.
Detecting sentence boundaries is however not so easy as simply breaking
sentences at these markers. The previously mentioned terminal markers
might have other purposes: full stops are often suffixed to
abbreviations or titles, in numbers, included in initialisms\footnote{Although
  the definition of initialism is ambiguous, this paper defines its use
  as an acronym (an abbreviation formed from initial components, such as
  ``sonar'' or ``FBI'') with full stops depicting elision (such as
  ``e.g.'', or ``K.G.B.'').}, or in embedded content\footnote{Embedded
  content in this paper refers to an external (non-grammatical) value
  embedded into a grammatical unit, such as a hyperlink or an emoticon.
  Note that these embedded values often consist of valid words and
  punctuation marks, but most always shouldn't be classified as such.}.
The interrogative- and exclamation points too can occur ambiguously,
most often in a quote (e.g., `\,``Of course!'', she screamed').
Disambiguation gets even harder when the previous mentioned exceptions
are in fact also a sentence boundary, such as in ``\ldots{}use the
feminine form of idem, ead.'' or in `\,``Of course!'', she screamed,
``I'll do it!''\,', where in both examples the last terminal markers end
their respective sentence.

\subsubsection{Word tokenisation}\label{word-tokenisation}

Like sentence tokenisation, word tokenisation is another elementary but
important stage in \gls{nlp} applications. Whether stemming, finding
phonetics, or \gls{pos} tagging, tokenising words is an important
precursory step.

Frequently, words are seen as everything that is not white space (i.e.,
spaces, tabs, feeds), thereby their boundaries denoted by those white
spaces\footnote{For example, Knwl counts words using this method
  \autocite{loadfive/knwl-source-code}.}. A smarter algorithm would also
treat punctuation marks as word boundaries, but such a general rule
would wrongly classify inter-word punctuation as not being part of
words\footnote{For example, Natural's tokenisers
  \autocite{NaturalNode/natural-source-code}.}. Many exceptions to this
rule exist, e.g., hyphenation points, colons, or elision (whether full
stops, ``e.g.''; apostrophes, the Dutch ``'s''; or slashes, ``N\slash A'').

\subsection{Applications}\label{applications}

In the previous section implementations were covered that solve
tokenisation stages in \gls{nlp} applications, such as Natural's word
tokenisers \autocite{NaturalNode/natural-source-code}. Concluded was
that these implementations are lacking. In this section several
implementations are covered that solve these tokenisation stages as part
of a larger natural language application.

\subsubsection{Sentiment Analysis}\label{sentiment-analysis}

Sentiment analysis is an \gls{nlp} task which is concerned with the
polarity (positive, negative) and subjectivity (objective, subjective)
of text. Its application could look as follows.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Detect language --- Often omitted if only a single language is
  supported.
\item
  Optionally tokenise sentences --- Different sentences could have
  different sentiments, tokenising them helps provide better outcomes.
\item
  Tokenise words --- Needed to compare with the database (see the last
  stage).
\item
  Optionally detect lemmas or stems --- Might help classification.
\item
  Detect sentiment --- Typically, sentiment analysers include a database
  mapping words, stems, or lemmas to their respective polarity and\slash or
  subjectivity\footnote{For example, the AFINN database mapping words to
    polarity \autocite{nielsen-finn-arup-afinn}.}.
\end{enumerate}

Many implementations exist for this task \autocites[e.g.,][]
{thinkroth/sentimental-source-code}{mileszim/sediment-source-code}
{thisandagain/sentiment-source-code}, many of which exclude non-alphabetic
characters form their definition of words, resulting in less than perfect
results\footnote{If fact, all of
  the researched implementations deploy lacking tokenisations steps.
  Controversial, as they each create unreachable code through their
  naivety: all tested libraries remove dashes from words, while words
  such as ``self-deluded'' are in the databases they use, but never
  reachable.}.

\subsubsection{Automatic Summarisation}\label{automatic-summarisation}

Automatic summarisation is an \gls{nlp} task concerned with reducing a
text in order to create a summary of the major points retaining the
original document. In contrast with sentiment analysers, implementations
of automatic summarisation algorithms on the web are less ubiquitously
available\footnote{For example, on the web only node-summary was found
  \autocite{jbrooksuk/node-summary-source-code}, in Scala textteaser was found
  \autocite{MojoJolo/textteaser-source-code}.}.

An example application for automatic summarisation could be as follows.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Detect language --- Often omitted if only a single language is
  supported.
\item
  Tokenise sentences --- Unless even finer grained control over the
  document is possible (tokenising phrases), sentences are the smallest
  unit that should stay intact in the resulting summary.
\item
  Tokenise words --- Used to calculate keywords (words which occur more
  often than expected by chance alone).
\item
  Automatic summarisation --- Ranking the tokenised grammatical units
  (sentences, phrases) and return the highest ranking units.
\end{enumerate}

Several factors can be used to determine whether or not a phrase or
sentence should be included from a summary:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Number of words (an ideal sentence is neither too long nor too short);
\item
  Number of keywords (words which occur more often than expected by
  chance alone in the whole text);
\item
  Number of words from the document's title it contains;
\item
  Position inside a paragraph (initial and final sentences are often
  more important than sentences buried somewhere in the middle of a
  paragraph).
\end{enumerate}

Some implementations only include keyword metrics
\autocite{jbrooksuk/node-summary-source-code}, others include all
aforementioned features \autocite{MojoJolo/textteaser-source-code}, or
even more advanced factors \autocite{summly}.

The only implementation working on the web
\autocite{jbrooksuk/node-summary-source-code}, takes a naive approach to
sentence tokenisation. For example, ignoring sentences terminated by an
exclamation point. Note that both other implementations, and many more,
use a whole different approach to sentence tokenisation: Corpora.

\subsection{Using Corpora for NLP}\label{using-corpora-for}

A corpus is a large, structured set of texts used for many linguistic
and \gls{nlp} tasks. Oftentimes, each item (most frequently words, but
some corpora include sentences, phrases, clauses, or other units) in a
corpus is annotated (``tagged'') with information about the item (for
example, \gls{pos} tags or lemmas).

These huge (often more than a million words\footnote{The Brown Corpus
  contains about a million words \autocite{francis-nelson-brown-corpus},
  the Google N-Gram Corpus contains 155 billion
  \autocite{brants-thorsten-google-ngram-corpus}.}) chunks of
information are the basis of many of the newer revolutions in \gls{nlp}
\autocite[see][]{mitkov-ruslan-ea-importance-corpora}. Supervised
learned\footnote{``{[}From{]} a set of labeled examples as training data
  \ldots{} {[}, make{]} predictions for all unseen points''
  \autocite{mohri-mehryar-foundations-machine-learning}.} parsing (in
\gls{nlp}, based on annotated corpora), is the direct antagonist of
rule-based parsing\footnote{A simple rule-based sentence tokeniser could
  be implemented as follows
  \autocite{attivio.com-doing-things-with-sentences}:

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    If it is a period, it ends a sentence;
  \item
    If the period is preceded by an abbreviation, it does not end a
    sentence;
  \item
    If the next token is capitalised, it ends a sentence.
  \end{itemize}}. Instead of defining rules, exceptions to these rules,
exceptions to these exceptions, and so on, supervised learning delegates
this task to machines, creating a more performant, scalable, program.

Generally, corpus linguistics have proven to be better in several ways
over rule-based linguistics. However, they do have their disadvantages:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Some rule-based approaches for pre- and post processing are still
  required;
\item
  Good training sets are required;
\item
  If the corpus was created from news articles, algorithms based on it
  won't fair so well on microblogs (e.g., Twitter posts);
\end{itemize}

Most important is the fact that supervised learning will not fare well
on the web: Loading corpora over the network each time a user request a
web page is unfeasible for most web sites or applications\footnote{Currently,
  almost no technologies exist for storing large datasets in a browser.
  The only exception to the rule is the \acrshort{html5} File System
  \acrshort{api}, but development has stopped
  \autocite{w3.org-filesystem-dead}.}.

Two viable alternative approaches exist for the web: the aforementioned
rule-based tokenisation, or connecting to a server over the network.

\subsection{Using a web API}\label{using-a-web}

Where the term \gls{api} stands for the interface between two programs,
the term is often defined in the context of web development as requests
(from a web browser), and responses (from a web server) over \gls{http}.
For example, Twitter has such a service to allow developers to list,
replace, create, and delete ``Tweets'' and other objects (users, images,
\&c.). In the context of this paper, the term Web \gls{api} is used for
the latter, whereas \gls{api} is used for any programming interface.

With the rise of the asynchronous web\footnote{Starting around 2000
  \gls{ajax} started to transform the web. Beforehand, websites only
  really changed when a user navigated to a new page. With \gls{ajax}
  however, new content arrived to users without the need for a full page
  refresh. One of the first examples are the Outlook Web App in 2000
  \autocite{technet-outlook-web-access} and Gmail in 2004
  \autocite{gmailblog-gmail-ajax}, both examples of how \gls{ajax} made
  the web feel more ``app-like''.}, supervised learning too became
widely available through web \glspl{api}
\autocites[e.g.,][]{textteaser-web-api}{wordnet-web-api}{textrazor-web-api}.
The new web \glspl{api} made it possible to implement supervised
learning techniques on the web, without the need for downloading a
corpus to your or your users' computer.

However, accessing \gls{nlp} web \glspl{api} over a network has
disadvantages. Most importantly the time involved when sending data over
the network (especially on mobile networks), the bandwidth used, and
heightened security risks.

\chapter{Design}\label{design}

\section{Architecture}\label{architecture}

The in this paper proposed solution to the problem of \gls{nlp} on the
client side is split up in multiple small proposals. Each proposal
solves a subproblem.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \gls{nlcst} --- Defines a standard for classifying grammatical units
  understandable for machines;
\item
  Parse-latin --- Classifies natural language according to \gls{nlcst};
\item
  \gls{textom} --- Provides an interface for analysing and manipulating
  output provided by parse-latin;
\item
  Retext --- Provides an interface for transforming natural language
  into an object model and exposes an interface for plugins.
\end{itemize}

The decoupled approach taken by the provided solution enables other
developers to provide their own software to replace one of the
sub-proposals. For example, other parties can create a parser for the
Chinese language and use it instead of parse-latin to classify natural
language according to \gls{nlcst}.

\subsection{Syntax: NLCST}\label{syntax}

To develop natural language tools in \gls{ecmascript}, an intermediate
representation of natural language is useful: instead of each module
defining their own representation of text, using a single syntax leads
to better results, interoperability, and performance.

The elements defined by \gls{nlcst} (Natural Language Concrete Syntax
Tree) are based on the the grammatical hierarchy, but by default do not
expose all its constituents\footnote{The grammatical hierarchy of text
  is constituted by words, phrases, clauses, and sentences.
  \glspl{nlcst} only implements the sentence and word constituents by
  default, although clauses and phrases could be provided by
  implementations.}. Additionally, more elements are provided to cover
other semantic units in natural language\footnote{Most notably,
  punctuation, symbol, and white space elements.}.

The definitions of exposed nodes were heavily based on other
specifications of syntax trees for manipulation on the web platform,
such as CSS, aptly named for the \acrshort{css} language
\autocite{reworkcss/css-source-code} or the Mozilla JavaScript
\acrshort{ast}, for \gls{ecmascript}
\autocite{mozilla.org-spidermonkey-parser_api}. Both implementations are
widely used. CSS by Rework \autocite{reworkcss/rework-source-code}, and
Mozilla JavaScript \gls{ast} by Esprima
\autocite{ariya/esprima-source-code}, Acorn
\autocite{marijnh/acorn-source-code}, and Escodegen
\autocite{constellation/escodegen-source-code}.

Note that the aforementioned syntax tree specifications are both
\glspl{ast}, whereas \gls{nlcst} is a a \gls{cst}. A concrete syntax
tree is a one-to-one mapping of source to result. All information stored
in the original input is also available through the resulting tree
\autocite{thegreenplace.net-abstract-concrete-syntax-trees}.

The information stored in \glspl{cst} is very verbose and could lead to
trees that are hard to work with. On the other hand, the fact that every
part of the input is housed in the tree, makes it easy for developers to
save the output or pass it on to other libraries for further processing.

See the appendices for a complete list of specified nodes of
\gls{nlcst}.

\subsection{Parser: Parse-latin}\label{parser-parse-latin}

\Gls{ecmascript} is used extensively. Because of this, many \gls{ecmascript} tools
are being developed. This includes tools for Natural Language
Processing. These \gls{ecmascript} tools however, when run on the client-side,
can not implement supervised learning based on corpora, and web
\gls{api} usage too is not ideal. Thus, a rule-based parser is needed to
tokenise text.

For creating such intermediate representations from Latin-script based
languages, parse-latin is presented in this paper\footnote{Whether
  Old-English, Icelandic, French, or even scripts slightly similar, such
  as Cyrillic, Georgian, or Armenian.}. As proof-of-concept, two other
libraries are also presented, parse-english and parse-dutch, using
parse-latin as a base and providing better support for several language
specific features, respectively English and Dutch.

By following \glspl{nlcst}, modules building on parse-latin may receive
better results or performance over implementing their own parsing tools.

By using the \gls{cst} as described by \gls{nlcst} and the parser as
described by parse-latin, the intermediate representation can be used by
developers to create independent modules.

Basically, parse-latin splits text into white space, word, and
punctuation tokens. parse-latin starts out with a pretty simple
definition, one that most other tokenisers use:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  A ``word'' is one or more letter or number characters;
\item
  A ``white space'' is one or more white space characters;
\item
  A ``punctuation'' is one or more of anything else;
\end{itemize}

Then, it manipulates and merges those tokens into a syntax tree, adding
sentences and paragraphs where needed.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Some punctuation marks are part of the word they occur in, e.g.,
  ``non-profit'', ``she's'', ``G.I.'', ``11:00'', ``N\slash A'';
\item
  Some full-stops do not mark a sentence end, e.g., ``1.'', ``e.g.'',
  ``id.'';
\item
  Although full-stops, question marks, and exclamation marks (sometimes)
  end a sentence, that end might not occur directly after the mark,
  e.g., ``.)'', '.``';
\end{itemize}

See the appendices for example output provided by the parse-latin
parser.

\subsubsection{parse-english}\label{parse-english}

parse-english has the same interface as parse-latin, but returns results
better suited for English natural language. For example:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Unit abbreviations (``tsp.'', ``tbsp.'', ``oz.'', ``ft.'', \&c.);
\item
  Time references (``sec.'', ``min.'', ``tues.'', ``thu.'', ``feb.'',
  \&c.);
\item
  Business Abbreviations (``Inc.'' and ``Ltd.'');
\item
  Social titles (``Mr.'', ``Mmes.'', ``Sr.'', \&c.);
\item
  Rank and academic titles (``Dr.'', ``Rep.'', ``Gen.'', ``Prof.'',
  ``Pres.'', \&c.);
\item
  Geographical abbreviations (``Ave.'', ``Blvd.'', ``Ft.'', ``Hwy.'',
  \&c.);
\item
  American state abbreviations (``Ala.'', ``Minn.'', ``La.'', ``Tex.'',
  \&c.);
\item
  Canadian province abbreviations (``Alta.'', ``Qué.'', ``Yuk.'', \&c.);
\item
  English county abbreviations (``Beds.'', ``Leics.'', ``Shrops.'',
  \&c.);
\item
  Common elision (omission of letters) (``\,`n'\,'', ``'o'', ``'em'',
  ``'twas'', ``'80s'', \&c.).
\end{itemize}

\subsubsection{parse-dutch}\label{parse-dutch}

parse-dutch has, just like parse-english, the same interface as
parse-latin, but returns results better suited for Dutch natural
language. For example:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Unit and time abbreviations (``gr.'', ``sec.'', ``min.'', ``ma.'',
  ``vr.'', ``vrij.'', ``febr'', ``mrt'', \&c.);
\item
  Many other common abbreviations: (``Mr.'', ``Mv.'', ``Sr.'', ``Em.'',
  ``bijv.'', ``zgn.'', ``amb.'', \&c.);
\item
  Common elision (omission of letters) (``d''', ``'n'', ``'ns'', ``'t'',
  ``'s'', ``'er'', ``'em'', ``'ie'', \&c.).
\end{itemize}

\subsection{Object Model: TextOM}\label{object-model}

To modify \gls{nlcst} nodes in \gls{ecmascript}, this paper proposes
\gls{textom}. \gls{textom} implements the nodes defined by \gls{nlcst},
but provides an object-oriented style\footnote{Object-oriented
  programming is a style of programming, where classes, instances,
  attributes, and methods are important.}. \gls{textom} was designed to
be similar to the \gls{dom}\footnote{See the appendices for a more
  information on the \gls{dom}.}, the mechanism used by browsers to
expose \gls{html} through \gls{ecmascript} to developers. Because of
\glspl{textom} likeness to the \gls{dom}, \gls{textom} is easy to learn
and familiar to the target audience.

\Gls{textom} provides events (a mechanism for detecting changes),
modification functionality (inserting, removing, and replacing children
into\slash from parents), and traversal (e.g., finding all words in a
sentence).

\Gls{nlcst} allows authors to extend the specification by defining their
own nodes, for example creating phrase or clause nodes. \Gls{textom}
allows for the same extension, and is build to work well with
``unknown'' node types.

\subsection{Natural Language System:
Retext}\label{natural-language-system-retext}

For natural language processing on the client side, this paper proposes
Retext. Retext combines a parser, such as parse-latin or parse-dutch,
with a manipulatable object model (\gls{textom}).

In addition, Retext provides a minimalistic plugin mechanism so
developers can create and publish plugins for others, and in turn can
use others' plugins inside their projects.

\chapter{Production}\label{production}

\section{Target Audience}\label{target-audience}

The audience that would benefit the most from such a solution are web
developers: programmers who specialise in creating software for the
world wide web. Web developers engage in client side development
(building the interface between a human and a machine on the web), and
sometimes also in server side development (building the interface
between the client side and a server). Typical areas of work consist of
programming in \gls{ecmascript}, marking up documents in \gls{html}, graphic
design through \gls{css}, creating a back end in node.js, \gls{php},
\&c., contacting a database through \gls{mongodb}, \gls{mysql}, and
more. In addition, many interdisciplinary skills are also of concern to
web developers, such as usability, accessibility, copywriting,
information architecture, optimisation.

\section{Use cases}\label{use-cases}

The use cases of the target audience, the front-end developer, in the
field of \gls{nlp} are many. Research for this paper found several use
cases, although it is expected many more could be defined. The tasks
below can be categorised into three broad fields: analysation,
manipulation, and creation.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The practitioner may intent summarise natural text (mostly
  analysation, potentially also manipulation);
\item
  The practitioner may intent to create natural language, e.g.,
  displaying the number of unread messages: ``You have 1 unread
  message,'' or ``You have 0 unread messages'' (creation);
\item
  The practitioner may intent to recognise sentiment in text: is a Tweet
  positive, negative, or spam? (analysation);
\item
  The practitioner may intent to replace so-called ``dumb'' punctuation
  with so-called ``smart'' punctuation, e.g., dumb quotation with (``)
  or (''); three dots with an ellipsis (\ldots{}), or two hyphens with
  an en-dash (--) (manipulation);
\item
  The practitioner may intent to count the number of certain elements in
  the grammatical hierarchy in a document, e.g, words, white space,
  punctuation, sentences, or paragraphs (analysation);
\item
  The practitioner may intent to recognise which language a given
  document is written in (analysation);
\item
  The practitioner may intent to find words based on a search term, with
  regards for the lemma (or stem) and\slash or phonetics (so that a search
  for ``smit'' also returns similar words, such as ``Schmidt'' or ``Smith'')
  (analysation and manipulation).
\end{itemize}

Natural Language Processing is a broad field concerned with the
interactions between computers and human languages, with bases in
computer science, artificial intelligence, and linguistics. \gls{nlp}
poses may challenges, but not every challenge challenge in the field is
of interest to the web developer---the developer enabling machines to
respond to humans through the world wide web. Most importantly, the more
academic areas of \gls{nlp} do not fit well with the goals of web
developers, e.g., speech recognition, optical character recognition,
text-to-speech transformation, translation, or machine learning.

\section{Requirements}\label{requirements}

The in the previous section covered use cases are required to be
reachable through the proposed solution. In addition, this section
covers other requirements to better suit the wishes of the target
audience.

\subsection{Open Source}\label{open-source}

To reach the target audience and validate its use, the software was
developed as open source software---software that is free for all to use
and redistribute. All projects were licensed under The \acrshort{mit}
License \autocite{opensource.org-licenses-mit}, a license which provides
rights for others to use, copy, modify, merge, publish, distribute,
sublicense, and\slash or sell copies of these projects.

In addition, the software was developed under the all-seeing-eye of the
GitHub community. GitHub is a hosted version control\footnote{Version
  control services manage revisions to documents, popularly used for
  controlling and tracking changes in software.} service with social
networking features. On GitHub, many web developers follow their peers
to track what they are working on, watch their favourite projects to get
notified of changes, and raise issues or feature requests.

\subsection{Performance}\label{performance}

The in this paper proposed implementations were produced with high
regards for performance. Performance includes the software having a
small file size in order to reach the client over the network with the
highest possible speed, but most importantly that the execution of code
should operate efficiently and at at high speeds.

\subsection{Testing}\label{testing}

With the development of the in this paper introduced software, testing
was given high priority. Testing, in software development, refers to
validating if software does what it is supposed to do, and can be
divided into multiple subgroups:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Unit testing --- Validation of each specific section of code;
\item
  Integration testing --- Validation of how programs work together;
\item
  System testing --- Validation of if system meets its requirements;
\item
  Acceptance testing --- Validation of the end product.
\end{itemize}

Great care was given to develop a full test suite with full coverage for
every program. Coverage, in software development, is a term used to
describe the amount of code tested by the test suite: full coverage
means every part of the code is reached by the tests.

Unit test were run through Mocha
\autocite{visionmedia/mocha-source-code}, coverage was detected with
Istanbul \autocite{gotwarlost/istanbul-source-code}.

\subsection{Code quality}\label{code-quality}

Great attention was given to code quality: how useful and readable for
both humans and machines the software is. Special focus was given to
consistency and clearness for humans.

\subsubsection{Suspicious Code and Bugs}\label{suspicious-code-and-bugs}

To detect bugs and suspicious code in the software, Eslint
\autocite{eslint/eslint-source-code} was used. The act of linting, in
computer programming, is a term used to describe static code analysis to
detect syntactic discrepancies without actually running the code. Eslint
was used because it provides a solid basic set of rules and enables
developers to create custom rules.

\subsubsection{Style}\label{style}

To enforce a consistent code style, in order to create software readable
for humans, \gls{jscs} was used
\autocite{mdevils/node-jscs-source-code}. \gls{jscs} provides rules for
allowing or disallowing patters such as white space at the end of a line
or camel cased variable names, or setting a maximum line length.
\gls{jscs} was chosen because it, just like the aforementioned Eslint,
provides a strong base set of rules. The rules chosen for the
development of the proposed software was set very strict to enforce all
code was written in the same manner.

\subsubsection{Commenting}\label{commenting}

Even when code is completely bug free, uses no hard-to-understand
shortcuts, and adheres to a strict style, it might still be hard to
understand for humans. The act of commenting code---describing what a
program does and why it accomplishes this in a certain way---is
important. On the other hand, commenting can also be to verbose, for
example when the code is duplicated in natural language.

\Gls{jsdoc} \autocite{google.com-clojure-compiler-jsdoc} is a markup
language for \gls{ecmascript} programs, allowing developers to embed
documentation in source code. Later various tools can be used to extract
the documentation and expose it separately from the original code.

Great care was given to annotate ``tricky'' source code inside the
software with comments, and to apply documentation inside the source
code through \gls{jsdoc}.

\subsection{Automation}\label{automation}

Great focus was given to develop using several automated \gls{ci}
environments. When suspicious, ambiguous, or buggy code was introduced
in the software, the error was automatically detected and in some cases
deployment was prevented.

Tools user were Code Climate \autocite{codeclimate.com} to detect
complex, duplicate, or bug-prone code, and Travis
\autocite{travis-ci.org} to validate all unit tests passed before
deploying the software.

\subsection{API Design}\label{design-1}

Interface design was given high priority for the development of the
proposed software. A clear interface of the software, according to
Joshua Bloch \autocite*{bloch-joshua-how-design-good-api-why-matters},
has the following characteristics:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Easy to learn;
\item
  Easy to use;
\item
  Hard to misuse;
\item
  Easy to read;
\item
  Easy to maintain;
\item
  Easy to extend;
\item
  Meeting its requirements;
\item
  Appropriate for the target audience.
\end{itemize}

In essence equal but worded differently are the characteristics of good
\gls{api} design according to the Qt Project
\autocite{qt-project.org-api-design-principles}, are as follows:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Be minimal;
\item
  Be complete;
\item
  Have clear and simple semantics;
\item
  Be intuitive;
\item
  Be easy to memorise;
\item
  Lead to readable code;
\end{itemize}

With the creation of the software these characteristics, and the in
their sources given examples, were taken into account.

\subsection{Installation}\label{installation}

Access both on the client side and on the server side to the software
was of importance during the development of the software. For the server
side on Node.js, \gls{npm}---the default package manager for the platform---is
the most popular. On the client side, many different package managers
exist, the most popular\footnote{Popularity here is simply defined as
  having the most search results on Google.} being Bower and Component.
To reach the target audience, in addition of making the whole source
available for download through Git and GitHub, \gls{npm}, Bower, and Component
were used.

\chapter{Validation}\label{validation}

The developed software was validated through two approaches. The design
and usability of the interface was validated by trying to solve the use
cases with the software. If the target audience actually wanted to use
the software was validated through enthusiasm showed by the open source
community.

\section{Plugins}\label{plugins}

More than fifteen (15) Retext plugins were created to validate how the
different projects integrated together, and how the system worked as a
whole.

The created plugins include tools for:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Transforming so-called dumb punctuation marks into more
  typographically correct punctuation marks
  \autocite*{wooorm/retext-smartypants-source-code};
\item
  Transforming emoji short-codes (:cat:) into real emoji
  \autocite*{wooorm/retext-emoji-source-code};
\item
  Detecting the direction of text
  \autocite*{wooorm/retext-directionality-source-code};
\item
  Detecting phonetics
  \autocite*{wooorm/retext-double-metaphone-source-code};
\item
  Detecting the stem of words
  \autocite*{wooorm/retext-porter-stemmer-source-code};
\item
  Detecting grammatical units
  \autocite*{wooorm/retext-visit-source-code};
\item
  Finding text, even misspelled
  \autocite*{wooorm/retext-search-source-code};
\item
  Detecting \gls{pos} tags
  \autocite*{wooorm/retext-pos-source-code};
\item
  Finding keywords and -phrases
  \autocite*{wooorm/retext-keywords-source-code};
\end{itemize}

The creation of these plugins brought several problems to light in the
developed software. These problems were then dealt with back and forth
between the software and the plugins. The software changed severely by
these fixed problems, which resulted in a more useable interface.

\section{Reception}\label{reception}

To validate if the target audience actually wanted to use the developed
software, several blogs and email newsletters were contacted to feature
Retext, either in the form of an article or as a simple link.

This resulted in several mentions on blogs
\autocite{dailyjs.com-natural-language-parsing-retext}, link roundups
\autocites[e.g., ][]{github.com-awesome-machine-learning}
{github.com-awesome-nodejs}, reddit  \autocites[e.g., ][]
{reddit.com-mention-1}{reddit.com-mention-2}{reddit.com-mention-3}. In turn,
these publications resulted in positive reactions, such as on Twitter
\autocites{twitter.com-mention-1}{twitter.com-mention-2}
{twitter.com-mention-3}{twitter.com-mention-4}{twitter.com-mention-5},
feedback \autocites{github.com-issue-1}{github.com-issue-2}, and fixes
\autocite{github.com-pull-request}. In addition, many web developers started
following the project on GitHub \autocite{github.com-stargazers}.

\chapter{Conclusion}\label{conclusion}

\section{Summary}\label{summary}

\section{Future Work}\label{future-work}

\section{Advice}\label{advice}
